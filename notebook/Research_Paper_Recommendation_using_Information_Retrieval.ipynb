{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f66724e8-647d-44aa-98ae-16720ac99f4b",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<p style=\"font-size:36px;text-align:center\"> <b>Research Paper Recommendation using Information Retrieval</b> </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98cdddf5-d955-412e-9fcd-70b4707f96e7",
   "metadata": {},
   "source": [
    "<h1>1. Business Problem</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e8ba7b-a01c-48a3-a9e4-0eaa999b8d9f",
   "metadata": {},
   "source": [
    "<h2> 1.1 Description </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805397b9-3d40-464d-83fa-a1419d298ef3",
   "metadata": {},
   "source": [
    "<p>Paper submission systems (CMT, OpenReview, etc.) require the users to upload paper titles and paper abstracts and then specify the subject areas their papers best belong to.</p>\n",
    "<p>\n",
    "Won't it be nice if such submission systems provided viable subject area suggestions as to where the corresponding papers could be best associated with?\n",
    "</p>\n",
    "<p>\n",
    "This dataset would allow developers to build baseline models that might benefit this use case. Data analysts might also enjoy analyzing the intricacies\n",
    "of different papers and how well their abstracts correlate to their noted categories. Additionally, we hope that the dataset will serve as a decent benchmark for building useful text classification systems.</p>\n",
    "<br>\n",
    "> Credits: Kaggle \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4a7bda-7d97-4986-8201-843636b02fb9",
   "metadata": {},
   "source": [
    "<h2> 1.2 Problem Statement </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80f4215-c653-4d51-9e7f-c3346250ca60",
   "metadata": {},
   "source": [
    "- We have data like Titles, Abstracts of different research papers.\n",
    "- These research papers are related with topics such as Machine Learning, Deep Learning, State of the Art Deep Learning models\n",
    "- These research papers are categorized into different subject areas.\n",
    "- Our task is when user provides Title and Abstract of any research paper, our model specify the areas in which that research paper belong to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcf2dec-13f8-4d33-8b05-6556dc00c6f1",
   "metadata": {},
   "source": [
    "<h2> 1.3 Sources/Useful Links</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cff0f8-bdf8-4cc0-8bbf-0fa589636d1b",
   "metadata": {},
   "source": [
    "- Source : https://www.kaggle.com/datasets/spsayakpaul/arxiv-paper-abstracts/data\n",
    "<br><br>____ Useful Links ____\n",
    "- Blog 1 : https://www.analyticsvidhya.com/blog/2021/06/part-20-step-by-step-guide-to-master-nlp-information-retrieval/\n",
    "- Blog 2 : https://scikit-learn.org/stable/modules/neighbors.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e729cab-53aa-423c-8a2f-f32c8698741e",
   "metadata": {},
   "source": [
    "<h2>1.4 Real world/Business Objectives </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5deba39e-e08b-409f-b486-a16b9046c949",
   "metadata": {},
   "source": [
    "- Along with the classification problem, we will further develop our model for the **Information Retrieval (IR) system**.\n",
    "- Our model will recommend relevant research papers to the user for which user asked the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61498a50-fad5-4032-8308-afae43a59a16",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "- Develop an advanced document retrieval system leveraging text classification and information retrieval techniques to improve the accuracy and efficiency of retrieving relevant documents from large datasets.</p>\n",
    "\n",
    "1. **Text Classification:** Automatically categorize documents into predefined categories based on their content.\n",
    "\n",
    "2. **Information Retrieval:** Enhance the retrieval process by ranking and retrieving documents based on their relevance to user queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe89aac-ce62-405d-94a8-3fe3bab64862",
   "metadata": {},
   "source": [
    "## What is **Information retrieval (IR)** system ?\n",
    " \n",
    "<p>1. It is the activity of obtaining information from resources that are relevant to an information need from a collection of those resources.</p><p>2. Searches can be based on full-text or other content-based indexing.</p>\n",
    "<p>3. Information retrieval is the science of searching for information in a document, searching for documents themselves, and also searching for the metadata that describes data, and for databases of texts, images or sounds.  Web search engines are the most visible IR applications.</p>\n",
    "<p>4. An information retrieval process begins when a user enters a query into the system. Queries are formal statements of information needs, for example search strings in web search engines. In information retrieval a query does not uniquely identify a single object in the collection. Instead, several objects may match the query, perhaps with different degrees of relevancy.</p>\n",
    "<p>5. User queries are matched against the database information. In information retrieval the results returned may or may not match the query, so results are typically ranked. This ranking of results is a key difference of information retrieval searching compared to database searching. </p>\n",
    "\n",
    "<p>6. Most IR systems compute a numeric score on how well each object in the database matches the query, and rank the objects according to this value. The top ranking objects are then shown to the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247365c9-9032-4395-9b23-4ee1507bd28d",
   "metadata": {},
   "source": [
    "<img src='relevant_output_about_information.jpg'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69cc6e0b-c731-44f0-99cc-59aa86e92124",
   "metadata": {},
   "source": [
    "<h1>2. Machine Learning Problem </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d66990-04b1-46fd-af4f-ce5627444335",
   "metadata": {},
   "source": [
    "<h3> 2.1 Data Overview </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0ab91ce-f529-4ed4-9dff-0a34e939ecd0",
   "metadata": {},
   "source": [
    "<p> \n",
    "- Data will be in a file arxiv_data_210930-054931.csv <br>\n",
    "- Train.csv contains 5 columns : titles, abstracts, terms <br>\n",
    "- Size of arxiv_data_210930-054931.csv - 70MB <br>\n",
    "- Number of rows in arxiv_data_210930-054931.csv = 56,181\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c1c141-a4e9-4cfa-af31-4e19d53958de",
   "metadata": {},
   "source": [
    "<h3> 2.2 Type of Machine Learning Problem </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b2c125-fb1a-42f7-85bb-8fd260fb1615",
   "metadata": {},
   "source": [
    "<p> It is a Multi-class classification problem. For a given research paper, we need to predict to which subject it belongs. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9d2975-5e8c-477f-afc9-366fefa78225",
   "metadata": {},
   "source": [
    "<h2> 3. Importing the Libraries </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20c972c5-39d0-4bbe-9299-e15e04c9c7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff2ad500-ed61-4b1f-a442-a88088cd36fc",
   "metadata": {},
   "source": [
    "**Lets import the article data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c57ccf0f-f03d-4b79-a620-cf499bddc941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Multi-Level Attention Pooling for Graph Neural...</td>\n",
       "      <td>Graph neural networks (GNNs) have been widely ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['cs.LG', 'cs.AI']</td>\n",
       "      <td>Decision Forests vs. Deep Networks: Conceptual...</td>\n",
       "      <td>Deep networks and decision forests (such as ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['cs.LG', 'cs.CR', 'stat.ML']</td>\n",
       "      <td>Power up! Robust Graph Convolutional Network v...</td>\n",
       "      <td>Graph convolutional networks (GCNs) are powerf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['cs.LG', 'cs.CR']</td>\n",
       "      <td>Releasing Graph Neural Networks with Different...</td>\n",
       "      <td>With the increasing popularity of Graph Neural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Recurrence-Aware Long-Term Cognitive Network f...</td>\n",
       "      <td>Machine learning solutions for pattern classif...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           terms  \\\n",
       "0                      ['cs.LG']   \n",
       "1             ['cs.LG', 'cs.AI']   \n",
       "2  ['cs.LG', 'cs.CR', 'stat.ML']   \n",
       "3             ['cs.LG', 'cs.CR']   \n",
       "4                      ['cs.LG']   \n",
       "\n",
       "                                              titles  \\\n",
       "0  Multi-Level Attention Pooling for Graph Neural...   \n",
       "1  Decision Forests vs. Deep Networks: Conceptual...   \n",
       "2  Power up! Robust Graph Convolutional Network v...   \n",
       "3  Releasing Graph Neural Networks with Different...   \n",
       "4  Recurrence-Aware Long-Term Cognitive Network f...   \n",
       "\n",
       "                                           abstracts  \n",
       "0  Graph neural networks (GNNs) have been widely ...  \n",
       "1  Deep networks and decision forests (such as ra...  \n",
       "2  Graph convolutional networks (GCNs) are powerf...  \n",
       "3  With the increasing popularity of Graph Neural...  \n",
       "4  Machine learning solutions for pattern classif...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"arxiv_data_210930-054931.csv\")\n",
    "\n",
    "# Show the top few papers\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b352b37-5494-4c3d-8b6f-9c9182535782",
   "metadata": {},
   "source": [
    "<h3> 3.1 Copy Original Data to new Dataframe </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b9202dd-b826-4d9f-bec4-47c7c71a9915",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d46fccef-e11e-4107-aeb4-aec8ee6f5a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56181, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a6220a-f8d3-4c33-8c07-7b77e42b809b",
   "metadata": {},
   "source": [
    "<h3> 3.2 Text Preprocessing </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc948ebd-5e25-43de-9b5f-504c7650dbf2",
   "metadata": {},
   "source": [
    "<h4> 3.2.1 Check for duplicate data </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "591b92a2-07b2-465a-8953-1a90b7942ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15054"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a65041-e9b5-40e4-ac64-ba5028754bef",
   "metadata": {},
   "source": [
    "<h4> 3.2.2 Remove duplicate data </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d06c76d-9dfe-4248-8c99-b42de5b5610c",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "162f47a4-222b-4c41-9f30-dd1888b8cc99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41127, 3)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee59033-3f65-4d5f-8083-bad26f397c2c",
   "metadata": {},
   "source": [
    "<h4> 3.2.3 Overview of data </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad5f5050-ed64-4c32-8d16-e90df276658a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Local Augmentation for Graph Neural Networks\n",
      "\n",
      "\n",
      "Abstract:  Data augmentation has been widely used in image data and linguistic data but\n",
      "remains under-explored on graph-structured data. Existing methods focus on\n",
      "augmenting the graph data from a global perspective and largely fall into two\n",
      "genres: structural manipulation and adversarial training with feature noise\n",
      "injection. However, the structural manipulation approach suffers information\n",
      "loss issues while the adversarial training approach may downgrade the feature\n",
      "quality by injecting noise. In this work, we introduce the local augmentation,\n",
      "which enhances node features by its local subgraph structures. Specifically, we\n",
      "model the data argumentation as a feature generation process. Given the central\n",
      "node's feature, our local augmentation approach learns the conditional\n",
      "distribution of its neighbors' features and generates the neighbors' optimal\n",
      "feature to boost the performance of downstream tasks. Based on the local\n",
      "augmentation, we further design a novel framework: LA-GNN, which can apply to\n",
      "any GNN models in a plug-and-play manner. Extensive experiments and analyses\n",
      "show that local augmentation consistently yields performance improvement for\n",
      "various GNN architectures across a diverse set of benchmarks. Code is available\n",
      "at https://github.com/Soughing0823/LAGNN.\n"
     ]
    }
   ],
   "source": [
    "print (\"Title: \", papers['titles'][10])\n",
    "print ('\\n')\n",
    "print (\"Abstract: \", papers['abstracts'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3223b35-eeec-4464-a513-8fbf79cd8770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Tensor Networks for Probabilistic Sequence Modeling\n",
      "\n",
      "\n",
      "Abstract:  Tensor networks are a powerful modeling framework developed for computational\n",
      "many-body physics, which have only recently been applied within machine\n",
      "learning. In this work we utilize a uniform matrix product state (u-MPS) model\n",
      "for probabilistic modeling of sequence data. We first show that u-MPS enable\n",
      "sequence-level parallelism, with length-n sequences able to be evaluated in\n",
      "depth O(log n). We then introduce a novel generative algorithm giving trained\n",
      "u-MPS the ability to efficiently sample from a wide variety of conditional\n",
      "distributions, each one defined by a regular expression. Special cases of this\n",
      "algorithm correspond to autoregressive and fill-in-the-blank sampling, but more\n",
      "complex regular expressions permit the generation of richly structured data in\n",
      "a manner that has no direct analogue in neural generative models. Experiments\n",
      "on sequence modeling with synthetic and real text data show u-MPS outperforming\n",
      "a variety of baselines and effectively generalizing their predictions in the\n",
      "presence of limited data.\n"
     ]
    }
   ],
   "source": [
    "print (\"Title: \", papers['titles'][100])\n",
    "print ('\\n')\n",
    "print (\"Abstract: \", papers['abstracts'][100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b437c741-4d17-4517-8bf9-798c96377adf",
   "metadata": {},
   "source": [
    "<h4> 3.2.4 Function to clean Abstracts and Titles </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3ee8be36-af6f-4157-8f4c-8886ff6ff71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that cleans text by removing '\\x0c' and '\\n' characters\n",
    "# as well as all non-alpha characters and finally converts everything\n",
    "# to lower case\n",
    "def clean_text(text):\n",
    "    stop_words = ['\\x0c', '\\n']\n",
    "    for i in stop_words:\n",
    "        text.replace(i, ' ')\n",
    "    clean_text = re.sub('[^a-zA-Z]+', ' ', text)\n",
    "    return clean_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55ca75ff-fa13-46d7-b435-e19e595bb5fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "      <th>clean_abstract</th>\n",
       "      <th>clean_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Multi-Level Attention Pooling for Graph Neural...</td>\n",
       "      <td>Graph neural networks (GNNs) have been widely ...</td>\n",
       "      <td>graph neural networks gnns have been widely us...</td>\n",
       "      <td>multi level attention pooling for graph neural...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>['cs.LG', 'cs.AI']</td>\n",
       "      <td>Decision Forests vs. Deep Networks: Conceptual...</td>\n",
       "      <td>Deep networks and decision forests (such as ra...</td>\n",
       "      <td>deep networks and decision forests such as ran...</td>\n",
       "      <td>decision forests vs deep networks conceptual s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>['cs.LG', 'cs.CR', 'stat.ML']</td>\n",
       "      <td>Power up! Robust Graph Convolutional Network v...</td>\n",
       "      <td>Graph convolutional networks (GCNs) are powerf...</td>\n",
       "      <td>graph convolutional networks gcns are powerful...</td>\n",
       "      <td>power up robust graph convolutional network vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>['cs.LG', 'cs.CR']</td>\n",
       "      <td>Releasing Graph Neural Networks with Different...</td>\n",
       "      <td>With the increasing popularity of Graph Neural...</td>\n",
       "      <td>with the increasing popularity of graph neural...</td>\n",
       "      <td>releasing graph neural networks with different...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Recurrence-Aware Long-Term Cognitive Network f...</td>\n",
       "      <td>Machine learning solutions for pattern classif...</td>\n",
       "      <td>machine learning solutions for pattern classif...</td>\n",
       "      <td>recurrence aware long term cognitive network f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           terms  \\\n",
       "0                      ['cs.LG']   \n",
       "1             ['cs.LG', 'cs.AI']   \n",
       "2  ['cs.LG', 'cs.CR', 'stat.ML']   \n",
       "3             ['cs.LG', 'cs.CR']   \n",
       "4                      ['cs.LG']   \n",
       "\n",
       "                                              titles  \\\n",
       "0  Multi-Level Attention Pooling for Graph Neural...   \n",
       "1  Decision Forests vs. Deep Networks: Conceptual...   \n",
       "2  Power up! Robust Graph Convolutional Network v...   \n",
       "3  Releasing Graph Neural Networks with Different...   \n",
       "4  Recurrence-Aware Long-Term Cognitive Network f...   \n",
       "\n",
       "                                           abstracts  \\\n",
       "0  Graph neural networks (GNNs) have been widely ...   \n",
       "1  Deep networks and decision forests (such as ra...   \n",
       "2  Graph convolutional networks (GCNs) are powerf...   \n",
       "3  With the increasing popularity of Graph Neural...   \n",
       "4  Machine learning solutions for pattern classif...   \n",
       "\n",
       "                                      clean_abstract  \\\n",
       "0  graph neural networks gnns have been widely us...   \n",
       "1  deep networks and decision forests such as ran...   \n",
       "2  graph convolutional networks gcns are powerful...   \n",
       "3  with the increasing popularity of graph neural...   \n",
       "4  machine learning solutions for pattern classif...   \n",
       "\n",
       "                                         clean_title  \n",
       "0  multi level attention pooling for graph neural...  \n",
       "1  decision forests vs deep networks conceptual s...  \n",
       "2  power up robust graph convolutional network vi...  \n",
       "3  releasing graph neural networks with different...  \n",
       "4  recurrence aware long term cognitive network f...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a column for cleaned Abstract and cleaned Title\n",
    "papers['clean_abstract'] = papers['abstracts'].apply(clean_text)\n",
    "papers['clean_title'] = papers['titles'].apply(clean_text)\n",
    "\n",
    "papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebede3fb-717b-4e67-a746-717947b95421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Recurrence-Aware Long-Term Cognitive Network for Explainable Pattern Classification\n",
      "\n",
      "\n",
      "Abstract:  Machine learning solutions for pattern classification problems are nowadays\n",
      "widely deployed in society and industry. However, the lack of transparency and\n",
      "accountability of most accurate models often hinders their safe use. Thus,\n",
      "there is a clear need for developing explainable artificial intelligence\n",
      "mechanisms. There exist model-agnostic methods that summarize feature\n",
      "contributions, but their interpretability is limited to predictions made by\n",
      "black-box models. An open challenge is to develop models that have intrinsic\n",
      "interpretability and produce their own explanations, even for classes of models\n",
      "that are traditionally considered black boxes like (recurrent) neural networks.\n",
      "In this paper, we propose a Long-Term Cognitive Network for interpretable\n",
      "pattern classification of structured data. Our method brings its own mechanism\n",
      "for providing explanations by quantifying the relevance of each feature in the\n",
      "decision process. For supporting the interpretability without affecting the\n",
      "performance, the model incorporates more flexibility through a quasi-nonlinear\n",
      "reasoning rule that allows controlling nonlinearity. Besides, we propose a\n",
      "recurrence-aware decision model that evades the issues posed by unique fixed\n",
      "points while introducing a deterministic learning method to compute the tunable\n",
      "parameters. The simulations show that our interpretable model obtains\n",
      "competitive results when compared to the state-of-the-art white and black-box\n",
      "models.\n"
     ]
    }
   ],
   "source": [
    "print (\"Title: \", papers['titles'][4])\n",
    "print ('\\n')\n",
    "print (\"Abstract: \", papers['abstracts'][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a28fcd7a-fe77-4dfc-af38-a8309bd470ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  recurrence aware long term cognitive network for explainable pattern classification\n",
      "\n",
      "\n",
      "Abstract:  machine learning solutions for pattern classification problems are nowadays widely deployed in society and industry however the lack of transparency and accountability of most accurate models often hinders their safe use thus there is a clear need for developing explainable artificial intelligence mechanisms there exist model agnostic methods that summarize feature contributions but their interpretability is limited to predictions made by black box models an open challenge is to develop models that have intrinsic interpretability and produce their own explanations even for classes of models that are traditionally considered black boxes like recurrent neural networks in this paper we propose a long term cognitive network for interpretable pattern classification of structured data our method brings its own mechanism for providing explanations by quantifying the relevance of each feature in the decision process for supporting the interpretability without affecting the performance the model incorporates more flexibility through a quasi nonlinear reasoning rule that allows controlling nonlinearity besides we propose a recurrence aware decision model that evades the issues posed by unique fixed points while introducing a deterministic learning method to compute the tunable parameters the simulations show that our interpretable model obtains competitive results when compared to the state of the art white and black box models \n"
     ]
    }
   ],
   "source": [
    "print (\"Title: \", papers['clean_title'][4])\n",
    "print ('\\n')\n",
    "print (\"Abstract: \", papers['clean_abstract'][4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd110623-5293-4899-a516-6acb02011c35",
   "metadata": {},
   "source": [
    "<h2> 4. Advanced Text Preprocessing </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b4be2-5ad7-44b2-ae24-1591e3342e94",
   "metadata": {},
   "source": [
    "- Once basic text Pre-processing done, we should do below Preprocessing steps.\n",
    "\n",
    "1. **Tokenization:**\n",
    "   * Tokenization is the process of splitting the given text into smaller pieces called tokens. Words, numbers, punctuation marks, and others can be considered as tokens. It is one of the initial steps of any NLP pipeline.\n",
    "\n",
    "   * Example - \\\n",
    "              Input: \"Tokenization is an important NLP task.\"  \\\n",
    "              Output: [\"Tokenization\", \"is\", \"an\", \"important\", \"NLP\", \"task\", \".\"\n",
    "</br>\n",
    "\n",
    "2. **Stemming:**\n",
    "\n",
    "   * <p>Stemming is the process of reducing a word to its word stem that affixes to suffixes and prefixes or to the roots of words known as a lemma.</p>\n",
    "\n",
    "   * <p>Stemming is a part of linguistic studies in morphology and artificial intelligence (AI) information retrieval and extraction. Stemming and AI knowledge extract meaningful information from vast sources like big data or the Internet since additional forms of a word related to a subject may need to be searched to get the best results. Stemming is also a part of queries and Internet search engines.</p>\n",
    "\n",
    "   * <p>Recognizing, searching and retrieving more forms of words returns more results. When a form of a word is recognized it can make it possible to return search results that otherwise might have been missed. That additional information retrieved is why stemming is integral to search queries and information retrieval.</p>\n",
    "\n",
    "\n",
    "   * <p>Applications of stemming are:\n",
    "     <p>1. Stemming is used in information retrieval systems like search engines.</p>\n",
    "     <p>2. It is used to determine domain vocabularies in domain analysis.</p>\n",
    "     <p>3. Stemming is desirable as it may reduce redundancy as most of the time the word stem and their inflected/derived words mean the same.</p>\n",
    "</br>\n",
    "\n",
    "3. **Lemmatization:**\n",
    "\n",
    "   * <p>The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form.</p>\n",
    "\n",
    "   * <p>However, the two words differ in their flavor. Stemming usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.</p>\n",
    "\n",
    "   * <p>Lemmatization usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the lemma .</p> \n",
    "\n",
    "</br>\n",
    "\n",
    "<p>The difference between Stemming and Lemmatization is that stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications.</p>\n",
    "\n",
    "</br>\n",
    "\n",
    "**For instance:**\r\n",
    "\r\n",
    "The word \"better\" has \"good\" as its lemma. This link is missed by stemming, as it requires a dictionary look-up.\r\n",
    "\r\n",
    "The word \"walk\" is the base form for word \"walking\", and hence this is matched in both stemming and lemmatisation.\r\n",
    "\r\n",
    "The word \"meeting\" can be either the base form of a noun or a form of a verb (\"to meet\") depending on the context, e.g., \"in our last meeting\" or \"We are meeting again tomorrow\". Unlike stemming, lemmatisation can in principle select the appropriate lemma depending on the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3793acfc-b053-4127-ac9a-4c3ca765219f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Use NLTK word_tokenize() and SnowballStemmer() to tokenize and stem document's Titles and Abstracts'''\n",
    "\n",
    "# Function that takes text, tokenizes it and returns list of stemmed tokens\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    stemmer = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
    "    return [i for i in [stemmer.stem(t) for t in tokens] if len(i) > 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0c1960-e675-4c06-be07-0e7de1a99f81",
   "metadata": {},
   "source": [
    "<h2> 5. Feature Extraction </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea22ed9-9ff2-42e7-8342-6a4c4bedb80f",
   "metadata": {},
   "source": [
    "<h3> 5.1 Different Feature Extraction Techniques </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d134a265-51df-425c-ba24-9c5cb1214449",
   "metadata": {},
   "source": [
    "<h2> 5.1.1 Bag of Words (BoW) </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12db2431-5d16-4f3e-ad1a-708f92107a67",
   "metadata": {},
   "source": [
    "* <p>The bag of words model is used for text representation and feature extraction in natural language processing and information retrieval tasks. It represents a text document as a multiset of its words, disregarding grammar and word order, but keeping the frequency of words. This representation is useful for tasks such as text classification, document similarity, and text clustering. </p>\n",
    "\n",
    "* <p>Bag-of-Words is one of the most fundamental methods to transform tokens into a set of features. The BoW model is used in document classification, where each word is used as a feature for training the classifier. For example, in a task of review based sentiment analysis, the presence of words like ‘fabulous’, ‘excellent’ indicates a positive review, while words like ‘annoying’, ‘poor’ point to a negative review . </p>\n",
    "    \n",
    "\n",
    "* <p>There are 3 steps while creating a BoW model : </p>\n",
    "  <p>1. The first step is text-preprocessing </p>\n",
    "  <p>2. The second step is to create a vocabulary of all unique words from the corpus. </p>\n",
    "  <p>3. In the third step, we create a matrix of features by assigning a separate column for each word, while each row corresponds to a review/document. This process is known as Text Vectorization. Each entry in the matrix signifies the presence(or absence) of the word in the review. We put 1 if the word is present in the review, and 0 if it is not present. </p>\n",
    "\n",
    "* <p>Let’s start with an example to understand by taking some sentences and generating vectors for those. </p>\n",
    "  <p>1. \"John likes to watch movies. Mary likes movies too\". </p>\n",
    "  <p>2. \"John also likes to watch football games\". </p>\n",
    "\n",
    "* <p>BoW vector representation for these two sentences will be </p>\n",
    "  <p>[1, 2, 1, 1, 2, 1, 1, 0, 0, 0] </p>\n",
    "  <p>[1, 1, 1, 1, 0, 0, 0, 1, 1, 1] </p>\n",
    "\n",
    "\n",
    "* **Issues of Bag of Words:**\n",
    "  <p>1. Lack of semantic information: As the bag of words model only considers individual words, it does not capture semantic relationships or the meaning of words in context. </p> \n",
    "  <p>2. Sparsity: For many applications, the bag of words representation of a document can be very sparse, meaning that most entries in the resulting feature vector will be zero. This can lead to issues with computational efficiency and difficulty in interpretability. </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb104b7-ff41-4ad2-9ca0-7c2b3bea5431",
   "metadata": {},
   "source": [
    "<h2> 5.1.2 TF-IDF (Term Frequency-Inverse Document Frequency) </h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728adc5a-bbc6-46cf-8891-b03d17cce2f1",
   "metadata": {},
   "source": [
    "<font face='georgia'>\n",
    "    \n",
    "   <h4><strong>What does tf-idf mean?</strong></h4>\n",
    "\n",
    "   <p>    \n",
    "Tf-idf stands for <em>term frequency-inverse document frequency</em>, and the tf-idf weight is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate how important a word is to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus. Variations of the tf-idf weighting scheme are often used by search engines as a central tool in scoring and ranking a document's relevance given a user query.\n",
    "</p>\n",
    "    \n",
    "   <p>\n",
    "One of the simplest ranking functions is computed by summing the tf-idf for each query term; many more sophisticated ranking functions are variants of this simple model.\n",
    "</p>\n",
    "    \n",
    "    \n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5db3c4-cadd-43ed-8fdc-98f067251b15",
   "metadata": {},
   "source": [
    "<font face='georgia'>\n",
    "    <h4><strong>How to Compute:</strong></h4>\n",
    "\n",
    "Typically, the tf-idf weight is composed by two terms: the first computes the normalized Term Frequency (TF), aka. the number of times a word appears in a document, divided by the total number of words in that document; the second term is the Inverse Document Frequency (IDF), computed as the logarithm of the number of the documents in the corpus divided by the number of documents where the specific term appears.\n",
    "\n",
    " <ul>\n",
    "    <li>\n",
    "<strong>TF:</strong> Term Frequency, which measures how frequently a term occurs in a document. Since every document is different in length, it is possible that a term would appear much more times in long documents than shorter ones. Thus, the term frequency is often divided by the document length (aka. the total number of terms in the document) as a way of normalization: <br>\n",
    "\n",
    "$TF(t) = \\frac{\\text{Number of times term t appears in a document}}{\\text{Total number of terms in the document}}.$\n",
    "</li>\n",
    "<li>\n",
    "<strong>IDF:</strong> Inverse Document Frequency, which measures how important a term is. While computing TF, all terms are considered equally important. However it is known that certain terms, such as \"is\", \"of\", and \"that\", may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing the following: <br>\n",
    "\n",
    "$IDF(t) = \\log_{e}\\frac{\\text{Total  number of documents}} {\\text{Number of documents with term t in it}}.$\n",
    "for numerical stabiltiy we will be changing this formula little bit\n",
    "$IDF(t) = \\log_{e}\\frac{\\text{Total  number of documents}} {\\text{Number of documents with term t in it}+1}.$\n",
    "</li>\n",
    "</ul>\n",
    "\n",
    "<br>\n",
    "<h4><strong>Example</strong></h4>\n",
    "<p>\n",
    "\n",
    "Consider a document containing 100 words wherein the word cat appears 3 times. The term frequency (i.e., tf) for cat is then (3 / 100) = 0.03. Now, assume we have 10 million documents and the word cat appears in one thousand of these. Then, the inverse document frequency (i.e., idf) is calculated as log(10,000,000 / 1,000) = 4. Thus, the Tf-idf weight is the product of these quantities: 0.03 * 4 = 0.12.\n",
    "</p>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f435faf-ae46-429c-98b7-fa725c48f806",
   "metadata": {},
   "source": [
    "<h3> 5.2 Create a tf-idf vectorizer using sklearn TfidfVectorizer </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd26d44-e67b-4e95-bb13-23652da3aaf4",
   "metadata": {},
   "source": [
    "1. First we create the vectorizer specifying the paramters\n",
    "    * max_df is the maximum allowable document frequency for a token this is set to 0.50 to include terms that appear in less than 50% of documents.\n",
    "    * min_df is the minimum allowable document frequency for a token.\n",
    "    * max_features sets the maximum number of features allowed and is set to an arbitrarily large number (i.e. 200,000) to ensure we capture at least as many features\n",
    "    * stop_words specifies the words/tokens to remove from the corpus\n",
    "    * use_idf enables reweighting each feature by its inverse-document-frequency when set to true\n",
    "    * tokenizer specifies which tokenizer to use, we want to tokenize and stem so we pass it our tokenized_and_stem() function we created above. The default tokenizer will tokenize words and include those greater than two characters in length.\n",
    "2. We then fit the vectorizer to our cleaned text using *vectorizer.fit_transform()*\n",
    "3. The output is a n*m matrix where n is the number of documents in our corpus and m is the number of features.\n",
    "4. We can inspect the features using *vectorizer.get_feature_names()*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a633d94e-ef29-4089-97aa-928adf4e41a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\rajan\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ed781aa-42bc-4e6c-b8f3-07a58704eb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the TfidfVectorizer from sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Create vectorizer for Abstracts, max_df is set to 0.5, we only want\n",
    "# to include terms that appear in less than 50% of the documents (i.e. rare terms)\n",
    "# we are selecting maximum 200,000 abstract features\n",
    "abstract_tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=1, max_features=200000,\n",
    "               stop_words='english', use_idf=True, tokenizer=tokenize_and_stem)\n",
    "\n",
    "# Create vectorizer for Title, max_df is set to 0.5, we only want\n",
    "# to include terms that appear in less than 50% of the documents (i.e. rare terms)\n",
    "# we are selecting maximum 100,000 title features\n",
    "title_tfidf_vectorizer = TfidfVectorizer(max_df=0.5, min_df=1, max_features=100000,\n",
    "               stop_words='english', use_idf=True, tokenizer=tokenize_and_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1317ca8d-0be9-4cc5-90e1-3e786968f23b",
   "metadata": {},
   "source": [
    "<h3> 5.3 Compute TF-IDF weights for Abstracts and Title </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "07e74bb3-402f-4186-b77d-370c17f7b975",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_weights_abs = abstract_tfidf_vectorizer.fit_transform(papers['clean_abstract'])\n",
    "tfidf_weights_title = title_tfidf_vectorizer.fit_transform(papers['clean_title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65ae2045-2fa4-406d-af41-c2af582e382c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41127, 40339)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_weights_abs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2020d5d3-6cef-451a-b540-8eb9d04cb1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41127, 13543)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_weights_title.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36bff2e8-b701-4b34-ab10-145b373d5ac7",
   "metadata": {},
   "source": [
    "<h3> 5.4 Get feature names for Abstract and Title models </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9401a65f-402b-43dc-9525-925f8886badd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names for Abstract and Title models\n",
    "tfidf_features_abs = abstract_tfidf_vectorizer.get_feature_names_out()\n",
    "tfidf_features_title = title_tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bcc385df-a738-4c71-9359-beb9028312f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['aaa', 'aaae', 'aaai', ..., 'zzh', 'zzz', 'zzzjzzz'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_features_abs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faedb9be-b40f-46b7-8dad-22d310918fec",
   "metadata": {},
   "source": [
    "<h3> 6. Function to get the top-k features associated with a document </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c84d1eec-6b30-4fed-8be5-6badb15bfaa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for returning the top_k features of an Abstract\n",
    "# or Title\n",
    "def get_top_features(rownum, weights, features, top_k=30):\n",
    "\n",
    "    #weights is a Sparse vector, hence need to convert into 2D matrix using toarray() function\n",
    "    weight_vec = weights.toarray()[rownum,:]\n",
    "\n",
    "    # weight_vec is tf-idf weight vector for particular row_num (abstract) \n",
    "    # where no. of columns = vocab size\n",
    "    \n",
    "    # We will sort this weight vector in decreasing order of the weights and will choose top_k weights\n",
    "    top_idx = np.argsort(weight_vec)[::-1][:top_k]\n",
    "\n",
    "    # We will return k-features which corresponds to the top_k weights\n",
    "    return [features[i] for i in top_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd4ff998-8a89-4a29-9c7a-147ee08ba68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['graph',\n",
       " 'fgn',\n",
       " 'node',\n",
       " 'lifelong',\n",
       " 'convert',\n",
       " 'gnn',\n",
       " 'turn',\n",
       " 'stream',\n",
       " 'featur',\n",
       " 'structur',\n",
       " 'independ',\n",
       " 'continu',\n",
       " 'wearabl',\n",
       " 'inherit',\n",
       " 'problem',\n",
       " 'classif',\n",
       " 'gnns',\n",
       " 'bridg',\n",
       " 'necessari',\n",
       " 'new',\n",
       " 'topolog',\n",
       " 'fashion',\n",
       " 'devic',\n",
       " 'assum',\n",
       " 'cnns',\n",
       " 'neural',\n",
       " 'classic',\n",
       " 'complet',\n",
       " 'signal',\n",
       " 'usual']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top k features of Abstract 5\n",
    "get_top_features(5, tfidf_weights_abs, tfidf_features_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "924a7f46-2721-4917-8a71-131ba4dbf80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conceptu',\n",
       " 'size',\n",
       " 'empir',\n",
       " 'forest',\n",
       " 'small',\n",
       " 'differ',\n",
       " 'similar',\n",
       " 'decis',\n",
       " 'sampl',\n",
       " 'deep',\n",
       " 'network',\n",
       " 'foraminifera',\n",
       " 'zstgan',\n",
       " 'forarbitrari',\n",
       " 'footwear',\n",
       " 'forbidden',\n",
       " 'forc',\n",
       " 'forcenet',\n",
       " 'ford',\n",
       " 'forag',\n",
       " 'foothil',\n",
       " 'footstep',\n",
       " 'footprint',\n",
       " 'forecastnet',\n",
       " 'footbal',\n",
       " 'footag',\n",
       " 'foot',\n",
       " 'fool',\n",
       " 'foodlogodet',\n",
       " 'food']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Top k features of Title 1\n",
    "get_top_features(1, tfidf_weights_title, tfidf_features_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06188c5f-d002-4c08-bdf3-8d015c601fb4",
   "metadata": {},
   "source": [
    "<h3> 7. Find Similar Abstracts and Titles using Nearest Neighbors model </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a96b0c-477e-4f4f-a29b-a89ed5eb6f20",
   "metadata": {},
   "source": [
    "- Using **Unsupervised k-NN** for information retrieval involves leveraging the k-nearest neighbors algorithm to find relevant items or documents without relying on labeled training data. Here’s how it can be applied in information retrieval:\n",
    "\n",
    "* In information retrieval, you often need to find items (like documents, images, or products) similar to a query item. Unsupervised k-NN can help by:\n",
    "\n",
    "1. **Constructing Feature Vectors:** Convert items and queries into feature vectors. For text, this might involve TF-IDF, word embeddings (e.g., Word2Vec, GloVe), or transformer-based embeddings (e.g., BERT). For images, this might involve feature vectors extracted using deep learning models.\n",
    "\n",
    "2. **Calculating Distances:** Use a distance metric (like Euclidean, cosine similarity, or Manhattan distance) to compute the similarity between the query vector and the vectors of the items in the dataset.\n",
    "\n",
    "3. **Finding Neighbors:** For a given query, find the k-nearest neighbors in the feature space, which are the k most similar items to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587c1002-f432-4b92-9cdb-bab4be70be19",
   "metadata": {},
   "source": [
    "<h4> 7.1 Build model to return 10 closest neighbors </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bfbb5e84-3c79-4c98-9934-b18ac2bdf9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Create the k-NN model using k=10\n",
    "nn_abs = NearestNeighbors(n_neighbors=10, algorithm='auto')\n",
    "nn_title = NearestNeighbors(n_neighbors=10, algorithm='auto')\n",
    "\n",
    "# Fit the models to the TF-IDF weights matrix\n",
    "nn_fitted_abs = nn_abs.fit(tfidf_weights_abs)\n",
    "nn_fitted_title = nn_title.fit(tfidf_weights_title)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aa3c89b-1030-478c-86a1-2d0caac20b29",
   "metadata": {},
   "source": [
    "<h4> 7.2 Function to return the top-k nearest papers </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ffd70a17-ac4f-495a-8513-2400b92f9535",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nearest_papers(row, kNNmodel, tfidf_weights, tfidf_features, papers):\n",
    "    # Get the top_k features of an Abstract or Title\n",
    "    keywords = get_top_features(row, tfidf_weights, tfidf_features)\n",
    "\n",
    "    # Get the Nearest 10 neighbors (similar papers) using k-NN\n",
    "    dist,idx = kNNmodel.kneighbors(tfidf_weights[row,:])\n",
    "\n",
    "    # Suggest top most 5 similar papers\n",
    "    idx = list(idx[0][1:6])\n",
    "    return {'papers':papers.iloc[idx], 'keywords':keywords}\n",
    "    #return papers.iloc[idx, 1:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46ab98e-e6b9-48fa-bbde-efa916444f31",
   "metadata": {},
   "source": [
    "<h3> 8. Return papers based on Abstract similarity </h3>\n",
    "\n",
    "Now that we have a function to return similar papers, we can use it to find papers with similar abstracts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c759d1b3-decf-42f6-83fa-f7bfc4bef591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title:  Local Augmentation for Graph Neural Networks\n",
      "\n",
      "\n",
      "Abstract:  Data augmentation has been widely used in image data and linguistic data but\n",
      "remains under-explored on graph-structured data. Existing methods focus on\n",
      "augmenting the graph data from a global perspective and largely fall into two\n",
      "genres: structural manipulation and adversarial training with feature noise\n",
      "injection. However, the structural manipulation approach suffers information\n",
      "loss issues while the adversarial training approach may downgrade the feature\n",
      "quality by injecting noise. In this work, we introduce the local augmentation,\n",
      "which enhances node features by its local subgraph structures. Specifically, we\n",
      "model the data argumentation as a feature generation process. Given the central\n",
      "node's feature, our local augmentation approach learns the conditional\n",
      "distribution of its neighbors' features and generates the neighbors' optimal\n",
      "feature to boost the performance of downstream tasks. Based on the local\n",
      "augmentation, we further design a novel framework: LA-GNN, which can apply to\n",
      "any GNN models in a plug-and-play manner. Extensive experiments and analyses\n",
      "show that local augmentation consistently yields performance improvement for\n",
      "various GNN architectures across a diverse set of benchmarks. Code is available\n",
      "at https://github.com/Soughing0823/LAGNN.\n"
     ]
    }
   ],
   "source": [
    "print (\"Title: \", papers['titles'][10])\n",
    "print ('\\n')\n",
    "print (\"Abstract: \", papers['abstracts'][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1b7f261d-5377-48df-b9d2-a6fa48e77726",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "      <th>clean_abstract</th>\n",
       "      <th>clean_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54045</th>\n",
       "      <td>['cs.LG', 'stat.ML']</td>\n",
       "      <td>Data Augmentation for Graph Neural Networks</td>\n",
       "      <td>Data augmentation has been widely used to impr...</td>\n",
       "      <td>data augmentation has been widely used to impr...</td>\n",
       "      <td>data augmentation for graph neural networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47255</th>\n",
       "      <td>['cs.LG', 'stat.ML']</td>\n",
       "      <td>Graph Analysis and Graph Pooling in the Spatia...</td>\n",
       "      <td>The spatial convolution layer which is widely ...</td>\n",
       "      <td>the spatial convolution layer which is widely ...</td>\n",
       "      <td>graph analysis and graph pooling in the spatia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19868</th>\n",
       "      <td>['cs.LG', 'cs.CV', 'stat.ML']</td>\n",
       "      <td>Data Augmentation Revisited: Rethinking the Di...</td>\n",
       "      <td>Data augmentation has been widely applied as a...</td>\n",
       "      <td>data augmentation has been widely applied as a...</td>\n",
       "      <td>data augmentation revisited rethinking the dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15123</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Graph Contrastive Learning with Adaptive Augme...</td>\n",
       "      <td>Recently, contrastive learning (CL) has emerge...</td>\n",
       "      <td>recently contrastive learning cl has emerged a...</td>\n",
       "      <td>graph contrastive learning with adaptive augme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29882</th>\n",
       "      <td>['cs.CV']</td>\n",
       "      <td>Adversarial Feature Augmentation for Unsupervi...</td>\n",
       "      <td>Recent works showed that Generative Adversaria...</td>\n",
       "      <td>recent works showed that generative adversaria...</td>\n",
       "      <td>adversarial feature augmentation for unsupervi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               terms  \\\n",
       "54045           ['cs.LG', 'stat.ML']   \n",
       "47255           ['cs.LG', 'stat.ML']   \n",
       "19868  ['cs.LG', 'cs.CV', 'stat.ML']   \n",
       "15123                      ['cs.LG']   \n",
       "29882                      ['cs.CV']   \n",
       "\n",
       "                                                  titles  \\\n",
       "54045        Data Augmentation for Graph Neural Networks   \n",
       "47255  Graph Analysis and Graph Pooling in the Spatia...   \n",
       "19868  Data Augmentation Revisited: Rethinking the Di...   \n",
       "15123  Graph Contrastive Learning with Adaptive Augme...   \n",
       "29882  Adversarial Feature Augmentation for Unsupervi...   \n",
       "\n",
       "                                               abstracts  \\\n",
       "54045  Data augmentation has been widely used to impr...   \n",
       "47255  The spatial convolution layer which is widely ...   \n",
       "19868  Data augmentation has been widely applied as a...   \n",
       "15123  Recently, contrastive learning (CL) has emerge...   \n",
       "29882  Recent works showed that Generative Adversaria...   \n",
       "\n",
       "                                          clean_abstract  \\\n",
       "54045  data augmentation has been widely used to impr...   \n",
       "47255  the spatial convolution layer which is widely ...   \n",
       "19868  data augmentation has been widely applied as a...   \n",
       "15123  recently contrastive learning cl has emerged a...   \n",
       "29882  recent works showed that generative adversaria...   \n",
       "\n",
       "                                             clean_title  \n",
       "54045        data augmentation for graph neural networks  \n",
       "47255  graph analysis and graph pooling in the spatia...  \n",
       "19868  data augmentation revisited rethinking the dis...  \n",
       "15123  graph contrastive learning with adaptive augme...  \n",
       "29882  adversarial feature augmentation for unsupervi...  "
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_papers(10, nn_fitted_abs, tfidf_weights_abs, tfidf_features_abs, papers)['papers']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85708234-7a21-4eb5-b131-b3babb6d3ffb",
   "metadata": {},
   "source": [
    "<h3> 9. Return papers based on Title similarity </h3>\n",
    "\n",
    "Now that we have a function to return similar papers, we can use it to find papers with similar Titles.es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8aa9670d-8114-4014-afa7-fa14b101fe68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>terms</th>\n",
       "      <th>titles</th>\n",
       "      <th>abstracts</th>\n",
       "      <th>clean_abstract</th>\n",
       "      <th>clean_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>54045</th>\n",
       "      <td>['cs.LG', 'stat.ML']</td>\n",
       "      <td>Data Augmentation for Graph Neural Networks</td>\n",
       "      <td>Data augmentation has been widely used to impr...</td>\n",
       "      <td>data augmentation has been widely used to impr...</td>\n",
       "      <td>data augmentation for graph neural networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36964</th>\n",
       "      <td>['cs.LG', 'stat.ML']</td>\n",
       "      <td>Non-Local Graph Neural Networks</td>\n",
       "      <td>Modern graph neural networks (GNNs) learn node...</td>\n",
       "      <td>modern graph neural networks gnns learn node e...</td>\n",
       "      <td>non local graph neural networks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52623</th>\n",
       "      <td>['cs.LG']</td>\n",
       "      <td>Graph Neural Networks with Local Graph Parameters</td>\n",
       "      <td>Various recent proposals increase the distingu...</td>\n",
       "      <td>various recent proposals increase the distingu...</td>\n",
       "      <td>graph neural networks with local graph parameters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>['cs.LG', 'eess.SP', 'stat.ML']</td>\n",
       "      <td>Graph Neural Network for Large-Scale Network L...</td>\n",
       "      <td>Graph neural networks (GNNs) are popular to us...</td>\n",
       "      <td>graph neural networks gnns are popular to use ...</td>\n",
       "      <td>graph neural network for large scale network l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11690</th>\n",
       "      <td>['cs.LG', 'stat.ML']</td>\n",
       "      <td>Reinforcement Learning using Augmented Neural ...</td>\n",
       "      <td>Neural networks allow Q-learning reinforcement...</td>\n",
       "      <td>neural networks allow q learning reinforcement...</td>\n",
       "      <td>reinforcement learning using augmented neural ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 terms  \\\n",
       "54045             ['cs.LG', 'stat.ML']   \n",
       "36964             ['cs.LG', 'stat.ML']   \n",
       "52623                        ['cs.LG']   \n",
       "134    ['cs.LG', 'eess.SP', 'stat.ML']   \n",
       "11690             ['cs.LG', 'stat.ML']   \n",
       "\n",
       "                                                  titles  \\\n",
       "54045        Data Augmentation for Graph Neural Networks   \n",
       "36964                    Non-Local Graph Neural Networks   \n",
       "52623  Graph Neural Networks with Local Graph Parameters   \n",
       "134    Graph Neural Network for Large-Scale Network L...   \n",
       "11690  Reinforcement Learning using Augmented Neural ...   \n",
       "\n",
       "                                               abstracts  \\\n",
       "54045  Data augmentation has been widely used to impr...   \n",
       "36964  Modern graph neural networks (GNNs) learn node...   \n",
       "52623  Various recent proposals increase the distingu...   \n",
       "134    Graph neural networks (GNNs) are popular to us...   \n",
       "11690  Neural networks allow Q-learning reinforcement...   \n",
       "\n",
       "                                          clean_abstract  \\\n",
       "54045  data augmentation has been widely used to impr...   \n",
       "36964  modern graph neural networks gnns learn node e...   \n",
       "52623  various recent proposals increase the distingu...   \n",
       "134    graph neural networks gnns are popular to use ...   \n",
       "11690  neural networks allow q learning reinforcement...   \n",
       "\n",
       "                                             clean_title  \n",
       "54045        data augmentation for graph neural networks  \n",
       "36964                    non local graph neural networks  \n",
       "52623  graph neural networks with local graph parameters  \n",
       "134    graph neural network for large scale network l...  \n",
       "11690  reinforcement learning using augmented neural ...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_nearest_papers(10, nn_fitted_title, tfidf_weights_title, tfidf_features_title, papers)['papers']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a45d60a-bcf0-45b8-a6e5-58e65dc66b31",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "1. Here both approaches means suggesting papers with Title similarity and with Abstract similarity gives good results.\n",
    "2. But with the Abstract similarity, model produces most accurate results and suggests better papers which are similar to the Query paper.\n",
    "3. Hence we will use the Abstract similarity model to suggest similar papers and we will suggest only titles of similar papers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb50e56-8bed-4371-837d-5df84ada4e65",
   "metadata": {},
   "source": [
    "<h3> 10. Let's find similar papers using Abstract similarity </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5464eeba-14a8-4ee2-b533-956dce5d7f6d",
   "metadata": {},
   "source": [
    "<h4> 10.1 Show the Titles of similar papers </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2ff5f041-c8bb-4b7e-bdb9-eb14b1850267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Adaptive Neural Message Passing for Inductive Learning on Hypergraphs\n",
      "\n",
      "Title: Hypergraph Modelling for Geometric Model Fitting\n",
      "\n",
      "Title: HyperSF: Spectral Hypergraph Coarsening via Flow-based Local Clustering\n",
      "\n",
      "Title: Noise-robust classification with hypergraph neural network\n",
      "\n",
      "Title: Hypergraph Convolution and Hypergraph Attention\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nearest_papers = find_nearest_papers(200, nn_fitted_abs, tfidf_weights_abs, tfidf_features_abs, papers)\n",
    "\n",
    "for paper in nearest_papers['papers']['titles']:\n",
    "    print(\"Title: \" + paper + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8492208-c6d0-4c56-a578-7239674074b3",
   "metadata": {},
   "source": [
    "<h4> 10.2 Show the Abstracts of similar papers </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b21a5b0f-6366-4e1a-8db2-1b05bfe449f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract: Graphs are the most ubiquitous data structures for representing relational\n",
      "datasets and performing inferences in them. They model, however, only pairwise\n",
      "relations between nodes and are not designed for encoding the higher-order\n",
      "relations. This drawback is mitigated by hypergraphs, in which an edge can\n",
      "connect an arbitrary number of nodes. Most hypergraph learning approaches\n",
      "convert the hypergraph structure to that of a graph and then deploy existing\n",
      "geometric deep learning methods. This transformation leads to information loss,\n",
      "and sub-optimal exploitation of the hypergraph's expressive power. We present\n",
      "HyperMSG, a novel hypergraph learning framework that uses a modular two-level\n",
      "neural message passing strategy to accurately and efficiently propagate\n",
      "information within each hyperedge and across the hyperedges. HyperMSG adapts to\n",
      "the data and task by learning an attention weight associated with each node's\n",
      "degree centrality. Such a mechanism quantifies both local and global importance\n",
      "of a node, capturing the structural properties of a hypergraph. HyperMSG is\n",
      "inductive, allowing inference on previously unseen nodes. Further, it is robust\n",
      "and outperforms state-of-the-art hypergraph learning methods on a wide range of\n",
      "tasks and datasets. Finally, we demonstrate the effectiveness of HyperMSG in\n",
      "learning multimodal relations through detailed experimentation on a challenging\n",
      "multimedia dataset.\n",
      "\n",
      "Abstract: In this paper, we propose a novel hypergraph based method (called HF) to fit\n",
      "and segment multi-structural data. The proposed HF formulates the geometric\n",
      "model fitting problem as a hypergraph partition problem based on a novel\n",
      "hypergraph model. In the hypergraph model, vertices represent data points and\n",
      "hyperedges denote model hypotheses. The hypergraph, with large and\n",
      "\"data-determined\" degrees of hyperedges, can express the complex relationships\n",
      "between model hypotheses and data points. In addition, we develop a robust\n",
      "hypergraph partition algorithm to detect sub-hypergraphs for model fitting. HF\n",
      "can effectively and efficiently estimate the number of, and the parameters of,\n",
      "model instances in multi-structural data heavily corrupted with outliers\n",
      "simultaneously. Experimental results show the advantages of the proposed method\n",
      "over previous methods on both synthetic data and real images.\n",
      "\n",
      "Abstract: Hypergraphs allow modeling problems with multi-way high-order relationships.\n",
      "However, the computational cost of most existing hypergraph-based algorithms\n",
      "can be heavily dependent upon the input hypergraph sizes. To address the\n",
      "ever-increasing computational challenges, graph coarsening can be potentially\n",
      "applied for preprocessing a given hypergraph by aggressively aggregating its\n",
      "vertices (nodes). However, state-of-the-art hypergraph partitioning\n",
      "(clustering) methods that incorporate heuristic graph coarsening techniques are\n",
      "not optimized for preserving the structural (global) properties of hypergraphs.\n",
      "In this work, we propose an efficient spectral hypergraph coarsening scheme\n",
      "(HyperSF) for well preserving the original spectral (structural) properties of\n",
      "hypergraphs. Our approach leverages a recent strongly-local max-flow-based\n",
      "clustering algorithm for detecting the sets of hypergraph vertices that\n",
      "minimize ratio cut. To further improve the algorithm efficiency, we propose a\n",
      "divide-and-conquer scheme by leveraging spectral clustering of the bipartite\n",
      "graphs corresponding to the original hypergraphs. Our experimental results for\n",
      "a variety of hypergraphs extracted from real-world VLSI design benchmarks show\n",
      "that the proposed hypergraph coarsening algorithm can significantly improve the\n",
      "multi-way conductance of hypergraph clustering as well as runtime efficiency\n",
      "when compared with existing state-of-the-art algorithms.\n",
      "\n",
      "Abstract: This paper presents a novel version of the hypergraph neural network method.\n",
      "This method is utilized to solve the noisy label learning problem. First, we\n",
      "apply the PCA dimensional reduction technique to the feature matrices of the\n",
      "image datasets in order to reduce the \"noise\" and the redundant features in the\n",
      "feature matrices of the image datasets and to reduce the runtime constructing\n",
      "the hypergraph of the hypergraph neural network method. Then, the classic\n",
      "graph-based semi-supervised learning method, the classic hypergraph based\n",
      "semi-supervised learning method, the graph neural network, the hypergraph\n",
      "neural network, and our proposed hypergraph neural network are employed to\n",
      "solve the noisy label learning problem. The accuracies of these five methods\n",
      "are evaluated and compared. Experimental results show that the hypergraph\n",
      "neural network methods achieve the best performance when the noise level\n",
      "increases. Moreover, the hypergraph neural network methods are at least as good\n",
      "as the graph neural network.\n",
      "\n",
      "Abstract: Recently, graph neural networks have attracted great attention and achieved\n",
      "prominent performance in various research fields. Most of those algorithms have\n",
      "assumed pairwise relationships of objects of interest. However, in many real\n",
      "applications, the relationships between objects are in higher-order, beyond a\n",
      "pairwise formulation. To efficiently learn deep embeddings on the high-order\n",
      "graph-structured data, we introduce two end-to-end trainable operators to the\n",
      "family of graph neural networks, i.e., hypergraph convolution and hypergraph\n",
      "attention. Whilst hypergraph convolution defines the basic formulation of\n",
      "performing convolution on a hypergraph, hypergraph attention further enhances\n",
      "the capacity of representation learning by leveraging an attention module. With\n",
      "the two operators, a graph neural network is readily extended to a more\n",
      "flexible model and applied to diverse applications where non-pairwise\n",
      "relationships are observed. Extensive experimental results with semi-supervised\n",
      "node classification demonstrate the effectiveness of hypergraph convolution and\n",
      "hypergraph attention.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for paper in nearest_papers['papers']['abstracts']: \n",
    "    print (\"Abstract: \"+ paper +\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef392810-8514-45dc-8a31-ee905dad2bc9",
   "metadata": {},
   "source": [
    "<h3> 11. Save and Load the Model </h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36b524d9-0677-4136-9814-3a0fcfef4e02",
   "metadata": {},
   "source": [
    "<h4> 11.1 Save Model and Feature weights </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0b5371e1-2804-41e1-ae81-7facaa9f04de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/tfidf_title_features.pkl']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(nn_fitted_abs , 'models/NN_abstract_model.pkl')\n",
    "joblib.dump(nn_fitted_title , 'models/NN_title_model.pkl')\n",
    "joblib.dump(tfidf_weights_abs , 'models/tfidf_abstract_weights.pkl')\n",
    "joblib.dump(tfidf_weights_title , 'models/tfidf_title_weights.pkl')\n",
    "joblib.dump(tfidf_features_abs , 'models/tfidf_abstract_features.pkl')\n",
    "joblib.dump(tfidf_features_title , 'models/tfidf_title_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ae62d7-43b9-44ef-ba18-f8ece8e9d529",
   "metadata": {},
   "source": [
    "<h4> 11.2 Load Model and Feature weights </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "72d2a345-52e5-42c5-92b3-0569dd7978c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_abstract_model = joblib.load('models/NN_abstract_model.pkl')\n",
    "tfidf_abstract_weights = joblib.load('models/tfidf_abstract_weights.pkl')\n",
    "tfidf_abstract_features = joblib.load('models/tfidf_abstract_features.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced67936-115c-4d4c-9a56-c118a28c76a4",
   "metadata": {},
   "source": [
    "<h4> 11.3 Recommendation of Papers </h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bc36b980-3e10-4b34-a0f2-1a86bf20343f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "query_paper_title = 'HyperSAGE: Generalizing Inductive Representation Learning on Hypergraphs'\n",
    "query_paper_index = papers.index[papers['titles'] == query_paper_title].tolist()\n",
    "print(query_paper_index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db65d31-5f94-44d9-87ca-53e9a9226da5",
   "metadata": {},
   "source": [
    "<h5> 11.3.1 Show the Titles of similar papers </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ccf05698-88a9-4d34-8a3c-eb5318e4e0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Adaptive Neural Message Passing for Inductive Learning on Hypergraphs\n",
      "\n",
      "Title: Hypergraph Modelling for Geometric Model Fitting\n",
      "\n",
      "Title: HyperSF: Spectral Hypergraph Coarsening via Flow-based Local Clustering\n",
      "\n",
      "Title: Noise-robust classification with hypergraph neural network\n",
      "\n",
      "Title: Hypergraph Convolution and Hypergraph Attention\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nearest_papers = find_nearest_papers(query_paper_index[0], NN_abstract_model, tfidf_abstract_weights, tfidf_abstract_features, papers)\n",
    "\n",
    "for paper in nearest_papers['papers']['titles']:\n",
    "    print(\"Title: \" + paper + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16fad41-0991-458f-b72d-cf39f3dfff41",
   "metadata": {},
   "source": [
    "<h5> 11.3.2 Show the Abstracts of similar papers </h5>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "06736ab5-8a75-4b20-a768-da1f66f444f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract: Graphs are the most ubiquitous data structures for representing relational\n",
      "datasets and performing inferences in them. They model, however, only pairwise\n",
      "relations between nodes and are not designed for encoding the higher-order\n",
      "relations. This drawback is mitigated by hypergraphs, in which an edge can\n",
      "connect an arbitrary number of nodes. Most hypergraph learning approaches\n",
      "convert the hypergraph structure to that of a graph and then deploy existing\n",
      "geometric deep learning methods. This transformation leads to information loss,\n",
      "and sub-optimal exploitation of the hypergraph's expressive power. We present\n",
      "HyperMSG, a novel hypergraph learning framework that uses a modular two-level\n",
      "neural message passing strategy to accurately and efficiently propagate\n",
      "information within each hyperedge and across the hyperedges. HyperMSG adapts to\n",
      "the data and task by learning an attention weight associated with each node's\n",
      "degree centrality. Such a mechanism quantifies both local and global importance\n",
      "of a node, capturing the structural properties of a hypergraph. HyperMSG is\n",
      "inductive, allowing inference on previously unseen nodes. Further, it is robust\n",
      "and outperforms state-of-the-art hypergraph learning methods on a wide range of\n",
      "tasks and datasets. Finally, we demonstrate the effectiveness of HyperMSG in\n",
      "learning multimodal relations through detailed experimentation on a challenging\n",
      "multimedia dataset.\n",
      "\n",
      "Abstract: In this paper, we propose a novel hypergraph based method (called HF) to fit\n",
      "and segment multi-structural data. The proposed HF formulates the geometric\n",
      "model fitting problem as a hypergraph partition problem based on a novel\n",
      "hypergraph model. In the hypergraph model, vertices represent data points and\n",
      "hyperedges denote model hypotheses. The hypergraph, with large and\n",
      "\"data-determined\" degrees of hyperedges, can express the complex relationships\n",
      "between model hypotheses and data points. In addition, we develop a robust\n",
      "hypergraph partition algorithm to detect sub-hypergraphs for model fitting. HF\n",
      "can effectively and efficiently estimate the number of, and the parameters of,\n",
      "model instances in multi-structural data heavily corrupted with outliers\n",
      "simultaneously. Experimental results show the advantages of the proposed method\n",
      "over previous methods on both synthetic data and real images.\n",
      "\n",
      "Abstract: Hypergraphs allow modeling problems with multi-way high-order relationships.\n",
      "However, the computational cost of most existing hypergraph-based algorithms\n",
      "can be heavily dependent upon the input hypergraph sizes. To address the\n",
      "ever-increasing computational challenges, graph coarsening can be potentially\n",
      "applied for preprocessing a given hypergraph by aggressively aggregating its\n",
      "vertices (nodes). However, state-of-the-art hypergraph partitioning\n",
      "(clustering) methods that incorporate heuristic graph coarsening techniques are\n",
      "not optimized for preserving the structural (global) properties of hypergraphs.\n",
      "In this work, we propose an efficient spectral hypergraph coarsening scheme\n",
      "(HyperSF) for well preserving the original spectral (structural) properties of\n",
      "hypergraphs. Our approach leverages a recent strongly-local max-flow-based\n",
      "clustering algorithm for detecting the sets of hypergraph vertices that\n",
      "minimize ratio cut. To further improve the algorithm efficiency, we propose a\n",
      "divide-and-conquer scheme by leveraging spectral clustering of the bipartite\n",
      "graphs corresponding to the original hypergraphs. Our experimental results for\n",
      "a variety of hypergraphs extracted from real-world VLSI design benchmarks show\n",
      "that the proposed hypergraph coarsening algorithm can significantly improve the\n",
      "multi-way conductance of hypergraph clustering as well as runtime efficiency\n",
      "when compared with existing state-of-the-art algorithms.\n",
      "\n",
      "Abstract: This paper presents a novel version of the hypergraph neural network method.\n",
      "This method is utilized to solve the noisy label learning problem. First, we\n",
      "apply the PCA dimensional reduction technique to the feature matrices of the\n",
      "image datasets in order to reduce the \"noise\" and the redundant features in the\n",
      "feature matrices of the image datasets and to reduce the runtime constructing\n",
      "the hypergraph of the hypergraph neural network method. Then, the classic\n",
      "graph-based semi-supervised learning method, the classic hypergraph based\n",
      "semi-supervised learning method, the graph neural network, the hypergraph\n",
      "neural network, and our proposed hypergraph neural network are employed to\n",
      "solve the noisy label learning problem. The accuracies of these five methods\n",
      "are evaluated and compared. Experimental results show that the hypergraph\n",
      "neural network methods achieve the best performance when the noise level\n",
      "increases. Moreover, the hypergraph neural network methods are at least as good\n",
      "as the graph neural network.\n",
      "\n",
      "Abstract: Recently, graph neural networks have attracted great attention and achieved\n",
      "prominent performance in various research fields. Most of those algorithms have\n",
      "assumed pairwise relationships of objects of interest. However, in many real\n",
      "applications, the relationships between objects are in higher-order, beyond a\n",
      "pairwise formulation. To efficiently learn deep embeddings on the high-order\n",
      "graph-structured data, we introduce two end-to-end trainable operators to the\n",
      "family of graph neural networks, i.e., hypergraph convolution and hypergraph\n",
      "attention. Whilst hypergraph convolution defines the basic formulation of\n",
      "performing convolution on a hypergraph, hypergraph attention further enhances\n",
      "the capacity of representation learning by leveraging an attention module. With\n",
      "the two operators, a graph neural network is readily extended to a more\n",
      "flexible model and applied to diverse applications where non-pairwise\n",
      "relationships are observed. Extensive experimental results with semi-supervised\n",
      "node classification demonstrate the effectiveness of hypergraph convolution and\n",
      "hypergraph attention.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for paper in nearest_papers['papers']['abstracts']: \n",
    "    print (\"Abstract: \"+ paper +\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
